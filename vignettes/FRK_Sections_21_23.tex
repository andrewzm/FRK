\subsection{The SRE model} \label{sec:SREModel}

Denote the spatial process of interest as $\{Y(\svec) : \svec \in D\}$, where $\svec$ indexes the location of $Y(\svec)$ in our domain of interest $D$. In what follows, we assume that $D$ is a spatial domain but extensions to spatio-temporal domains are natural within the framework (Section \ref{sec:ST}). Consider the classical spatial statistical model,
\begin{equation*}
Y(\svec) = \tvec(\svec)^\top\alphab + \upsilon(\svec) + \xi(\svec); \quad \svec \in D,
\end{equation*}
where, for $\svec \in D$, $\tvec(\svec)$ is a vector of spatially referenced covariates, $\alphab$ is a vector of regression coefficients, $\upsilon(\svec)$ is a small-scale, spatially correlated random effect, and $\xi(\svec)$ is a fine-scale random effect that is `almost' spatially uncorrelated. It is natural to let $\E(\upsilon(\cdot)) = \E(\xi(\cdot)) = 0$. Define $\lambda(\cdot) \equiv \upsilon(\cdot) + \xi(\cdot)$, so that $\expect(\lambda(\cdot)) = 0$. It is the structure of the process $\upsilon(\cdot)$ in terms of a linear combination of a fixed number of spatial basis functions that defines the SRE model for $\lambda(\cdot)$:
$$
\lambda(\svec) = \sum_{l=1}^r \phi_l(\svec)\eta_l + \xi(\svec);\quad \svec \in D,
$$
\noindent where $\etab \equiv (\eta_1,\dots,\eta_r)^\top$ is an $r$-variate random vector, and $\phib(\cdot) \equiv (\phi_1(\cdot),\dots,\phi_r(\cdot))^\top$ is an $r$-dimensional vector of pre-specified spatial basis functions. Sometimes, $\phib(\cdot)$ contains basis functions of multiple resolutions (e.g., wavelets), they may or may not be orthogonal, and they may or may not have compact support. The basis functions chosen should be able to adequately reconstruct realisations of $Y(\cdot)$; an empirical spectral-based approach that can ensure this is discussed in \cite{Zammit_2012}.

In order to cater for different observation supports $\{B_j\}$ (defined below), it is convenient to assume a discretised domain of interest $D^G \equiv \{A_i \subset D: i = 1,\dots,N\}$ that is made up of $N$ small, non-overlapping basic areal units or BAUs \citep{Nguyen_2012}, and $D = \bigcup_{i=1}^N A_i$. The set $D^G$ of BAUs is a discretisation, or `tiling,' of the original domain $D$, and typically $N \gg r$. The process $\{Y(\svec): \svec \in D\}$ is then averaged over the BAUs, giving the vector $\Yvec = (Y_i : i = 1,\dots,N)^\top$, where

\begin{equation}\label{eq:Yi0}
Y_i \equiv \frac{1}{|A_i|}\int_{A_i} Y(\svec) \intd \svec; \quad i = 1,\dots,N,
\end{equation}

\noindent and $N$ is the number of BAUs. At this BAU level,
\begin{equation} \label{eq:Yi1}
Y_i = \tvec_i^\top\alphab + \upsilon_i + \xi_i,
\end{equation}
\noindent where for $i = 1,\dots, N,$ $\tvec_i \equiv \frac{1}{|A_i|}\int_{A_i} \tvec(\svec) \intd \svec$, $\upsilon_i\equiv \frac{1}{|A_i|}\int_{A_i} \upsilon(\svec) \intd \svec$, and $\xi_i$ is specified below. The SRE model specifies that the small-scale random variation is $\upsilon(\cdot) = \phib(\cdot)^\top\etab,$ and hence in terms of the discretisation onto $D^G$,

%Using basis-function decomposition, we let $\upsilon_1(\svec) = (\phi_1(\svec),\dots,\phi_r(\svec))\etab \equiv \phib(\svec)^\top\etab$. Then,

$$
\upsilon_i = \left(\frac{1}{|A_i|}\int_{A_i}\phib(\svec)\intd \svec\right)^\top \etab; \quad i = 1,\dots,N,
$$
\noindent so that $\upsilonb = \Smat\etab$, where $\Smat$ is the $N \times r$ matrix defined as follows:
\begin{equation}\label{eq:S_integral}
\Smat \equiv \left(\frac{1}{|A_i|}\int_{A_i}\phib(\svec)\intd\svec : i = 1,\dots,N\right)^\top.
\end{equation}

In \pkg{FRK}, we assume that $\etab$ is an $r$-dimensional Gaussian vector with mean zero and $r \times r$ covariance matrix $\Kmat$, and estimation of $\Kmat$ is based on likelihood methods; we denote this variant of FRK as FRK-V (where recall that `V' stands for `vanilla'). If some structure is imposed on $\var(\etab)$ in terms of parameters $\varthetab$, then $\Kmat = \Kmat_\circ(\varthetab)$ and $\varthetab$ needs to be estimated; we denote this variant as FRK-M (where recall that `M' stands for `model'). Frequently, the resolution of the BAUs is sufficiently fine, and the basis functions are sufficiently smooth, so that $\Smat$ can be approximated:
\begin{equation}\label{eq:Sapprox}
\Smat \approx \left(\phib(\svec_i) : i = 1,\dots,N\right)^\top,
\end{equation}
where $\{\svec_i: i = 1,\dots,N\}$ are the centroids of the BAUs. Since small BAUs are always assumed, this approximation is used throughout \pkg{FRK}.

In \pkg{FRK}, we do not directly model $\xi(\svec)$, since we are only interested in its discretised version. Rather, we assume that $\xi_i \equiv \frac{1}{|A_i|}\int_{A_i} \xi(\svec)\intd\svec$ has a Gaussian distribution with mean zero and variance
%assume that the BAU is the smallest unit of measure. Thus, the fine-scale variation is a constant random effect in each BAU, but uncorrelated across the BAUs. Since a BAU has finite area, the fine-scale variation is not a nugget effect in the classical sense. We thus model $\xi_i$ to be Gaussian with mean zero and uncorrelated with variance
\begin{equation*}
\var(\xi_i) = \sigma^2_\xi \varv_{\xi,i},
\end{equation*}
where $\sigma^2_\xi$ is a parameter to be estimated, and the weights $\{\varv_{\xi,1},\dots,\varv_{\xi,N}\}$ are known and represent heteroscedasticity. These weights are typically generated from domain knowledge; they may, for example, correspond to topographical features such as terrain roughness \citep{Zammit_2015}. Since we specified $\xi(\cdot)$ to be `almost' spatially uncorrelated, it is reasonable to assume that the variables representing the discretised fine-scale variation, $\{\xi_i: i = 1,\dots,N\}$, are uncorrelated. From \eqref{eq:Yi1}, we can write
\begin{equation}\label{eq:SRE_Y}
\Yvec = \Tmat\alphab + \Smat\etab + \xib,
\end{equation}
where $\Tmat \equiv (\tvec_i: i = 1,\dots,N)^\top$, $\xib \equiv (\xi_i : i = 1,\dots,N)^\top$, and $\var(\xib) \equiv \sigma^2_\xi \Vmat_\xi$, for known $\Vmat_\xi \equiv \diag(\varv_{\xi,1},\dots,\varv_{\xi,N})$.

We now  assume that the hidden (or latent) process, $Y(\cdot)$, is observed with $m$ footprints (possibly overlapping) spanning one or more BAUs, where typically $m \gg r$ (note that both $m > N$ and $N \ge m$ are possible). We thus define the observation domain as $D^O \equiv \{ \cup_{i \in c_j} A_i : j = 1,\dots,m \}$, where $c_j$ is a non-empty set in $2^{\{1,\dots,N\}}$, the power set of $\{1,\dots,N\}$, and $m = |D^O|$.  For illustration, consider the simple case of  the discretised domain being made up of three BAUs. Then $D^G = \{A_1,A_2,A_3\}$ and, for example, $D^O = \{B_1, B_2\}$, where $B_1 = A_1 \cup A_2$ (i.e., $c_1 = \{1,2\}$) and $B_2 = A_3$ (i.e., $c_2 = \{3\}$). Catering for different footprints is important for remote sensing applications in which satellite-instrument footprints can widely differ \citep[e.g.,][]{Zammit_2015}.

Each $B_j \in D^O$ is either a BAU or a union of BAUs. Measurement of $\Yvec$ is imperfect: We define the measurement process as noisy measurements of the process averaged over the footprints
\begin{equation}\label{eq:meas_process}
Z_j \equiv Z(B_j) = \left(\frac{\sum_{i =1}^N Y_i w_{ij}}{\sum_{i=1}^N w_{ij}}\right) + \left(\frac{\sum_{i =1}^N \delta_i w_{ij}}{\sum_{i=1}^N w_{ij}}\right) + \epsilon_j; \quad B_j \in D^O,
\end{equation}
where the weights,
$$ w_{ij} = |A_i|\mathbb{I}(A_i \subset B_j); \quad i = 1,\dots,N;~~j = 1,\dots, m; ~~B_j \in D^O,$$
depend on the areas of the BAUs, and $\mathbb{I}(\cdot)$ is the indicator function. Currently, in \pkg{FRK}, BAUs of equal area are assumed, but we give \eqref{eq:meas_process} in its most general form.   The random quantities $\{\delta_i\}$ and $\{\epsilon_i\}$ capture the imperfections of the measurement. Better known is the measurement-error component $\epsilon_i$, which is assumed to be mean-zero Gaussian distributed. The component $\delta_i$ captures any bias in the measurement at the BAU level, which has the interpretation of an intra-BAU systematic error. These systematic errors are BAU-specific, that is, the $\{\delta_i\}$ are uncorrelated with mean zero and variance
%Any bias in the measurement is represented by the spatially independent systematic error $\deltab \equiv (\delta_1,\dots,\delta_N)^\top$ with individual variances,
%weight the process averages according to the areas of the BAUs, $\epsilon_j$ is Gaussian measurement error, $\mathbb{I}(\cdot)$ is the indicator function, and $\delta_i$ is spatially-dependent \emph{systematic error} in the measurement.
\begin{equation*}
\var(\delta_i) = \sigma^2_\delta \varv_{\delta,i},
\end{equation*}
where $\sigma^2_\delta$ is a parameter to be estimated, and $\{\varv_{\delta,1},\dots,\varv_{\delta,N}\}$  represent known heteroscedasticity.

We assume that $\Yvec$ and $\deltab$ are independent. We also assume that the observations are conditionally independent, when conditioned on $\Yvec$ and $\deltab$. Equivalently, we assume that the measurement errors $\{\epsilon_j: j = 1,\dots,m\}$ are independent with $\var(\epsilon_i) = \sigma^2_\epsilon\varv_{\epsilon,i}$.

We represent the data as $\Zvec \equiv (Z_j : j = 1,\dots,m)^\top$. Then, since each element in $D^O$ is the union of subsets of $D^G$, one can construct a matrix
$$
\Cmat_Z \equiv \left(\frac{w_{ij}}{\sum_{l=1}^N w_{lj}} : i = 1,\dots,N; j = 1,\dots,m\right),
$$
such that
\begin{equation*}
\Zvec = \Cmat_Z\Yvec + \Cmat_Z\deltab +  \epsilonb,
\end{equation*}
\noindent where the three components are independent, $\epsilonb \equiv (\epsilon_j : j = 1,\dots,m)^\top$, and $\var(\epsilonb) = \Sigmamat_\epsilon \equiv \sigma^2_\epsilon\Vmat_\epsilon \equiv \sigma^2_\epsilon \diag(\varv_{\epsilon,1},\dots,\varv_{\epsilon,m})$ is an $m \times m$ diagonal covariance matrix. The matrix $\Sigmamat_\epsilon$ is assumed known from the properties of the measurement. If it is not known, $\Vmat_\epsilon$ is fixed to $\Imat$ and $\sigma^2_\epsilon$ is estimated using variogram techniques \citep{Kang_2009}. Notice that the rows of  the matrix $\Cmat_Z$ sum to 1.

% If these errors are also identically distributed, then $\sigma^2_\epsilon = \var(\epsilon_j), j = 1,\dots,m$, can be estimated from the data provided that $\Vmat_\xi$ and $\Vmat_\delta \equiv \diag(\varv_{\delta,1},\dots,\varv_{\delta,N})$ are not proportional to each other.


It will be convenient to re-write
\begin{equation}\label{eq:Z_collapsed}
\Zvec = \Tmat_Z\alphab + \Smat_Z\etab + \xib_Z + \deltab_Z + \epsilonb,
\end{equation}
where $\Tmat_Z \equiv \Cmat_Z \Tmat$, $\Smat_Z \equiv \Cmat_Z \Smat$, $\xib_Z \equiv \Cmat_Z \xib$, $\deltab_Z \equiv \Cmat_Z \deltab$, $\var(\xib_Z) = \sigma^2_\xi\Vmat_{\xi,Z} \equiv \sigma^2_\xi\Cmat_Z\Vmat_{\xi}\Cmat_Z^\top$, $\var(\deltab_Z) = \sigma^2_\delta\Vmat_{\delta,Z} \equiv \sigma^2_\delta\Cmat_Z\Vmat_\delta\Cmat_Z^\top$, and where $\Vmat_\delta \equiv \diag(\varv_{\delta,1},\dots,\varv_{\delta,N})$ is known. Then, recalling that $\expect(\etab) = \zerob$ and $\expect(\xib_Z) = \expect(\deltab_Z) = \expect(\epsilonb) = \zerob$,
\begin{align*}
\expect(\Zvec) &=\Tmat_Z\alphab,\\
\var(\Zvec) &= \Smat_Z\Kmat\Smat_Z^\top + \sigma^2_\xi\Cmat_Z\Vmat_{\xi}\Cmat_Z^\top +  \sigma^2_\delta\Cmat_Z\Vmat_\delta\Cmat_Z^\top + \sigma^2_\epsilon\Vmat_\epsilon.
\end{align*}
In practice, it is not always possible for each $B_j$ to include entire BAUs. For simplicity, in \pkg{FRK} we assume that the observation footprint overlaps a BAU if and only if the BAU centroid lies within the footprint. Frequently, point-referenced data is included in $\Zvec$. In this case, each data point is attributed to a specific BAU and it is possible to have multiple observations of the process defined on the same BAU.


We collect the unknown parameters in the set $\thetab \equiv \{\alphab, \sigma^2_\xi, \sigma^2_\delta, \Kmat\}$ for FRK-V and $\thetab_\circ \equiv \{\alphab, \sigma^2_\xi, \sigma^2_\delta, \varthetab\}$ for FRK-M for which $\Kmat = \Kmat_\circ(\varthetab)$; their estimation is the subject of Section \ref{sec:estimation}. If the parameters in $\thetab$ or $\thetab_\circ$ are known, an inversion that uses the Sherman--Woodbury identity \citep{Henderson_1981} allows spatial prediction at any BAU in $D^G$. Estimates of $\thetab$ are substituted into these spatial predictors to yield FRK-V. Similarly, estimates of $\thetab_\circ$ substituted into the spatial-prediction equations yield FRK-M.

In \pkg{FRK}, we allow the prediction set $D^P$ to be as flexible as $D^O$; specifically, $D^P \subset \{ \cup_{i \in \tilde{c}_k} A_i : k = 1,\dots,N_P \}$, where $\tilde{c}_k$ is a non-empty set in $2^{\{1,\dots,N\}}$ and $N_P$ is the number of prediction areas. We can thus predict both at the individual BAU level or averages over an area spanning multiple BAUs, and these prediction regions may overlap. This is an important change-of-support feature of \pkg{FRK}. We provide the FRK equations in Section \ref{sec:prediction}.

\subsection{Parameter estimation using an EM algorithm} \label{sec:estimation}

In all its generality, parameter estimation with the model of Section \ref{sec:SREModel} is problematic due to confounding between $\deltab$ and $\xib$. In \pkg{FRK}, the user thus needs to choose between modelling the intra-BAU systematic errors (in which case $\sigma^2_\xi$ is fixed to 0) or the process' fine-scale variation (in which case $\sigma^2_\delta$ is fixed to 0).
%where the fine-scale variation sits, either in the observation model \eqref{eq:meas_process} (in which case $\sigma^2_\xi$ is fixed to 0) or in the process model \eqref{eq:Yi1} (in which case $\sigma^2_\delta$ is fixed to 0).
We describe below the estimation procedure for the latter case; due to symmetry, the estimation equations of the former case can be simply obtained by replacing the subscript $\xi$ with $\delta$. However, which case is chosen by the user has a considerable impact on the prediction equations for $Y$ (Section \ref{sec:prediction}). Recall that the measurement-error covariance matrix $\Sigmamat_\epsilon$ is assumed known from measurement characteristics, or estimated using variogram techniques prior to estimating the remaining parameters described below. For conciseness, in this section we use $\thetab$ to denote the parameters in both FRK-V and FRK-M, only distinguishing when necessary.

We carry out parameter estimation using an expectation maximisation (EM) algorithm \citep[similar to][]{Katzfuss_2011,Nguyen_2014} with \eqref{eq:Z_collapsed} as our model. Define the complete-data likelihood $L_c(\thetab) \equiv [\etab,\Zvec \mid \thetab]$ (with $\xib_Z$ integrated out), where $[~\cdot~]$ denotes the probability distribution of its argument.  The EM algorithm proceeds by first computing the conditional expectation (conditional on the data) of the complete-data log-likelihood at the current parameter estimates (the E-step) and, second, maximising this function with respect to the parameters (the M-step). In mathematical notation, in the E-step the function
\begin{equation*}
Q(\thetab \mid \thetab^{(l)}) \equiv \expect(\ln L_c(\thetab) \mid \Zvec,\thetab^{(l)}),
\end{equation*}
is found for some current estimate $\thetab^{(l)}$. In the M-step, the updated parameter estimates
\begin{equation*}
\thetab^{(l+1)} = \argmax_\thetab Q(\thetab \mid \thetab^{(l)}),
\end{equation*}
are found.

The E-step boils down to finding the conditional distribution of $\etab$ at the current parameter estimates. One can use standard results in Gaussian conditioning \citep[e.g., ][ Appendix A]{Rasmussen_2006}  to show from the joint distribution, $[\etab,\Zvec \mid \thetab^{(l)}]$, that
\begin{equation*}
\etab \mid \Zvec,\thetab^{(l)} \sim \Gau(\muvec_\eta^{(l)},\Sigmamat_\eta^{(l)}),
\end{equation*}
where
\begin{align*}
\muvec_\eta^{(l)} &= \Sigmamat_\eta^{(l)} \Smat_Z^\top\left(\Dmat_Z^{(l)}\right)^{-1}\left(\Zvec - \Tmat_Z\alphab^{(l)}\right), \\
\Sigmamat_\eta^{(l)} &= \left(\Smat_Z^\top\left(\Dmat_Z^{(l)}\right)^{-1}\Smat_Z + \left(\Kmat^{(l)}\right)^{-1}\right)^{-1},
\end{align*}
where $\Dmat_Z^{(l)} \equiv (\sigma^2_\xi)^{(l)} \Vmat_{\xi,Z} + \Sigmamat_\epsilon,$ and where $\Kmat^{(l)}$ is defined below.

The update for $\alphab$ is
\begin{equation}
\alphab^{(l+1)} = \left(\Tmat_Z^\top \left(\Dmat_Z^{(l+1)}\right)^{-1} \Tmat_Z\right)^{-1}\Tmat_Z^\top\left(\Dmat_Z^{(l+1)}\right)^{-1}\left(\Zvec - \Smat_Z \muvec_\eta^{(l)}\right). \label{eq:alpha}
\end{equation}
In FRK-V, the update for $\Kmat^{(l+1)}$ is

\begin{equation*}
\Kmat^{(l+1)} = \Sigmamat_\eta^{(l)} + \muvec_\eta^{(l)} \muvec_\eta^{(l)^\top},
\end{equation*}

\noindent while in FRK-M, where recall that $\Kmat = \Kmat_\circ(\varthetab)$, the update is
\begin{equation*}
\varthetab^{(l+1)} = \argmax_{\varthetab} \ln\left| \Kmat_\circ(\varthetab)^{-1}\right| - \tr\left(\Kmat_\circ(\varthetab)^{-1}\left(\Sigmamat_\eta^{(l)} + \muvec_\eta^{(l)} \muvec_\eta^{(l)^\top}\right)\right),
\end{equation*}
\noindent which is numerically optimised using the function \code{optim} with $\varthetab^{(l)}$ as the initial vector.

The update for $\sigma_\xi^2$ requires the solution to
\begin{equation} \label{eq:sigma2d}
\tr((\Sigmamat_{\epsilon} + (\sigma^2_\xi)^{(l+1)}\Vmat_{\xi,Z})^{-1}\Vmat_{\xi,Z}) = \tr((\Sigmamat_{\epsilon} + (\sigma^2_\xi)^{(l+1)}\Vmat_{\xi,Z})^{-1}\Vmat_{\xi,Z}(\Sigmamat_{\epsilon} + (\sigma^2_\xi)^{(l+1)}\Vmat_{\xi,Z})^{-1}\Omegab),
\end{equation}
where
\begin{equation} \label{eq:Omegab}
\Omegab \equiv \Smat_Z \Sigmamat_\eta^{(l)} \Smat_Z^\top + \Smat_Z \muvec_\eta^{(l)}\muvec_\eta^{(l)^\top} \Smat_Z^\top - 2\Smat_Z\muvec_\eta^{(l)}(\Zvec - \Tmat_Z\alphab^{(l+1)})^\top + (\Zvec - \Tmat_Z\alphab^{(l+1)})(\Zmat - \Tmat_Z\alphab^{(l+1)})^\top.
\end{equation}

\noindent The solution to \eqref{eq:sigma2d}, namely $(\sigma^2_\xi)^{(l+1)}$, is found numerically using \code{uniroot} after \eqref{eq:alpha} is substituted into \eqref{eq:Omegab}. Then $\alphab^{(l+1)}$ is found by substituting $(\sigma^2_\xi)^{(l+1)}$ into \eqref{eq:alpha}. Computational simplifications are possible when $\Vmat_{\xi,Z}$ and $\Sigmamat_\epsilon$ are diagonal, since then only the diagonal of $\Omegab$ needs to be computed. Further simplifications are possible when $\Vmat_{\xi,Z}$ and $\Sigmamat_\epsilon$ are proportional to the identity matrix, with constants of proportionality $\gamma_1$ and $\gamma_2$, respectively. In this case,
\begin{equation*}
(\sigma^2_\xi)^{(l+1)} = \frac{1}{\gamma_1} \left(\frac{\tr(\Omegab)}{m} - \gamma_2 \right),
\end{equation*}
where recall that $m$ is the dimension of the data vector $\Zvec$ and $\alphab^{(l+1)}$ is, in this special case, the ordinary-least-squares estimate given $\muvec_\eta^{(l)}$ (see \eqref{eq:alpha}). These simplifications are used by \pkg{FRK} whenever possible.

Convergence of the EM algorithm is assessed using the (incomplete-data) log-likelihood function at each iteration,
\begin{equation*}
\ln \left[\Zvec \mid \alphab^{(l)}, \Kmat^{(l)}, (\sigma^2_\xi)^{(l)}\right] = -\frac{m}{2}\ln 2\pi -\frac{1}{2}\ln \left|\Sigmamat_Z^{(l)}\right| - \frac{1}{2}(\Zvec - \Tmat_Z\alphab^{(l)})^\top(\Sigmamat_Z^{(l)})^{-1}(\Zvec - \Tmat_Z\alphab^{(l)}),
\end{equation*}
where
\begin{equation*}
\Sigmamat_Z^{(l)} = \Smat_Z \Kmat^{(l)} \Smat_Z^\top + \Dmat_Z^{(l)},
\end{equation*}
and recall that $\Dmat_{Z}^{(l)} \equiv (\sigma_\xi^2)^{(l)}\Vmat_{\xi,Z} + \Sigmamat_\epsilon$. Efficient computation of the log-likelihood is facilitated through the use of the Sherman--Morrison--Woodbury matrix identity and a matrix-determinant lemma \citep[e.g.,][]{Henderson_1981}. Specifically, the operations
\begin{align*}
\left(\Sigmamat_Z^{(l)}\right)^{-1} &= \left(\Dmat_Z^{(l)}\right)^{-1} - \left(\Dmat_Z^{(l)}\right)^{-1} \Smat_Z \left[\left(\Kmat^{(l)}\right)^{-1} + \Smat^\top_Z \left(\Dmat_Z^{(l)}\right)^{-1}\Smat_Z\right]^{-1}\Smat_Z^\top\left(\Dmat_Z^{(l)}\right)^{-1},\\
\left| \Sigmamat_Z^{(l)}  \right| &= \left| \left(\Kmat^{(l)}\right)^{-1} + \Smat_Z^\top\left(\Dmat_Z^{(l)}\right)^{-1} \Smat_Z \right| \left|\Kmat^{(l)} \right| \left|\Dmat_Z^{(l)}\right|,%\label{eq:determinant}
\end{align*}
ensure that we only deal with vectors of length $m$ and matrices of size $r \times r$, where typically the fixed rank $r \ll m,$ the dataset size.

%To prove \eqref{eq:determinant}, start from the right-hand-side, noting that $| \Sigmamat_Z^{(l)}|  = | \Imat + \Smat_Z^\top(\Dmat_Z^{-1})^{(l)} \Smat_Z \Kmat^{(l)}| |\Dmat_Z^{(l)} |.$ Applying Sylvester's determinant identity, $| \Imat + \Smat_Z^\top(\Dmat_Z^{-1})^{(l)} \Smat_Z \Kmat^{(l)}| \equiv | \Imat +  \Smat_Z \Kmat^{(l)}\Smat_Z^\top(\Dmat_Z^{-1})^{(l)}|$, we see that $| \Sigmamat_Z^{(l)}| =  |\Smat_Z \Kmat^{(l)} \Smat_Z^\top + \Dmat_Z^{(l)}|$, as required.

\subsection{Prediction} \label{sec:prediction}

The prediction task is to make inference on the hidden $Y$-process over a set of prediction regions $D^P$. Consider the process $\{Y_P(\tilde{B}_k): k = 1,\dots,N_P\}$, which is derived from the $Y$ process and, similar to the observations, is constructed using the BAUs $\{A_i: i = 1,\dots,N\}$. Here, $N_P$ is the number of areas at which spatial prediction takes place, and is equal to $|D^P|$. Then,

\begin{equation*}
Y_{P,k} \equiv Y_{P}(\tilde{B}_k) = \left(\frac{\sum_{i =1}^N Y_i \tilde w_{ik}}{\sum_{i=1}^N \tilde w_{ik}}\right); \quad \tilde{B}_k \in D^P,
\end{equation*}
where the weights are
$$ \tilde w_{ik} = |A_i|\mathbb{I}(A_i \subset \tilde{B}_k); \quad i = 1,\dots,N;~~k = 1,\dots, N_P; ~~\tilde{B}_k \in D^P.$$

Define $\Yvec_P \equiv (Y_{P,k} : k = 1,\dots,N_P)^\top$. Then, since each element in $D^P$ is the union of subsets of $D^G$, one can construct a matrix,
\begin{equation}\label{eq:CP}
\Cmat_P \equiv \left(\frac{\tilde w_{ik}}{\sum_{l=1}^N \tilde w_{lk}} : i = 1,\dots,N; k = 1,\dots,N_P\right),
\end{equation}
the rows of which sum to 1, such that
\begin{equation*}
\Yvec_P = \Cmat_P\Yvec = \Tmat_P\alphab + \Smat_P\etab + \xib_P,
\end{equation*}
where $\Tmat_P \equiv \Cmat_P \Tmat$, $\Smat_P \equiv \Cmat_P \Smat$, $\xib_P \equiv \Cmat_P \xib$ and $\var(\xib_P) = \sigma^2_\xi\Vmat_{\xi,P} \equiv \sigma^2_\xi\Cmat_P\Vmat_\xi\Cmat_P^\top$.  As with the observations, the prediction regions $\{\tilde{B}_k \}$ may overlap. In practice, it may not always be possible for each $\tilde{B}_k$ to include entire BAUs. In this case, we assume that a prediction region contains a BAU if and only if the BAU centroid lies within the region.

Let $l^*$ denote the EM iteration number at which convergence is deemed to have been reached. The final estimates are then
$$\widehat\muvec_\eta \equiv \muvec_\eta^{(l^*)},~~ \widehat\Sigmamat_\eta \equiv \Sigmamat_\eta^{(l^*)},~~ \widehat\alphab \equiv \alphab^{(l^*)}, ~~\widehat\Kmat \equiv \Kmat^{(l^*)}, ~~\widehat\sigma^2_\xi \equiv (\sigma^2_\xi)^{(l^*)}, \textrm{~~and~~} \widehat\sigma^2_\delta \equiv (\sigma^2_\delta)^{(l^*)}.$$
Recall from Section \ref{sec:estimation} that the user needs to attribute fine-scale variation at the BAU level to either the measurement process or the hidden process $Y$. This leads to the following two cases.

{\bf Case 1:} $\sigma^2_\xi = 0$ and estimate $\sigma^2_\delta$. The prediction vector $\widehat\Yvec_P$ and covariance matrix $\Sigmamat_{Y_P\mid Z}$, corresponding to the first two moments from the predictive distribution $[\Yvec_P \mid \Zvec]$ when $\sigma^2_\xi = 0,$ are
\begin{align*}
\widehat\Yvec_P \equiv \expect(\Yvec_P \mid \Zvec) &= \Tmat_P\widehat\alphab + \Smat_P\widehat\muvec_\eta, \\%\label{eq:smooth1}\\
\Sigmamat_{Y_P \mid Z} \equiv \var(\Yvec_P\mid \Zvec) &= \Smat_P \widehat\Sigmamat_\eta\Smat_P^\top. %\label{eq:smooth2}
\end{align*}
 Under the assumptions taken, $[\Yvec_P \mid \Zvec]$ is a $\Gau(\widehat\Yvec_P,\Sigmamat_{Y_P \mid Z})$ distribution. Note that all calculations are made after substituting in the EM-estimated parameters, and that $\widehat\sigma^2_\delta$ is present in the estimated parameters.

{\bf Case 2:} $\sigma^2_\delta = 0$ and estimate $\sigma^2_\xi$  (Default). To cater for arbitrary observation and prediction support, we predict $\Yvec_P$ by first carrying out prediction over the full vector $\Yvec$, that is, at the BAU level, and then transforming linearly to obtain $\widehat\Yvec_P$ through the use of the matrix $\Cmat_P$. It is easy to see that if $\widehat\Yvec$ is an optimal (squared-error-loss matrix criterion) predictor of $\Yvec$, then $\Amat\widehat\Yvec$ is an optimal predictor of $\Amat\Yvec$, where $\Amat$ is any matrix with $N$ columns.

Let $\Wvec \equiv (\etab^\top,\xib^\top)^\top$ and $\Pimat \equiv (\Smat,\Imat)$. Then \eqref{eq:SRE_Y} can be re-written as $\Yvec = \Tmat\alphab + \Pimat\Wvec$, and
\begin{align}
\widehat\Yvec \equiv \expect(\Yvec \mid \Zvec) &= \Tmat\widehat\alphab + \Pimat\widehat\Wvec, \nonumber \\
\Sigmamat_{Y \mid Z} \equiv \var(\Yvec \mid \Zvec) &= \Pimat \Sigmamat_W\Pimat^\top, \label{eq:Sigma_YZ}
\end{align}
for
\begin{align*}
\widehat\Wvec &\equiv \Sigmamat_W\Pimat^\top\Cmat_Z^\top\Sigmamat_\epsilon^{-1}(\Zvec - \Tmat_Z\widehat\alphab),\\%\label{eq:What} \\
\Sigmamat_W &\equiv \left(\Pimat^\top \Cmat_Z^\top \Sigmamat_\epsilon^{-1} \Cmat_Z \Pimat + \Lambdamat^{-1}\right)^{-1},\\% \label{eq:Winv}
\end{align*}
and the block-diagonal matrix $\Lambdamat \equiv \textrm{bdiag}(\widehat\Kmat,\widehat\sigma^{2}_\xi\Vmat_{\xi})$, where $\textrm{bdiag}(\cdot)$ returns a block diagonal matrix of its matrix arguments. Note that all calculations are made after substituting in the EM-estimated parameters.

For both Cases 1 and 2 it follows that $\widehat\Yvec_P = \expect(\Yvec_P \mid \Zvec) = \Cmat_P\widehat\Yvec$ and
\begin{equation}\label{eq:YvecP}
\Sigmamat_{Y_P\mid Z} = \var(\Yvec_P \mid \Zvec) = \Cmat_P \Sigmamat_{Y \mid Z} \Cmat_P^\top.
\end{equation}

Note that for Case 2 we need to obtain predictions for $\xib_P$ which, unlike those for $\etab$, are not a by-product of the EM algorithm of \ref{sec:estimation}. Sparse-matrix operations are used to facilitate the computation of \eqref{eq:YvecP} when possible.% is not available at this stage. Since we generally wish to predict at locations other than where we have observations, this quantity is rarely equal to $\xib_Z$; for this reason, $\xib_Z$ was not predicted in Section \ref{sec:estimation}.

